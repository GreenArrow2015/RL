import tensorflow as tf
from tensorflow import keras
tf.reset_default_graph()
sess = tf.InteractiveSession()
tf.keras.backend.set_session(sess)


from keras.layers import Conv2D, Dense, Flatten

class DQNAgent:
    def __init__(self, name, state_shape, n_actions, epsilon=0, reuse=False):
        """A simple DQN agent"""
        with tf.variable_scope(name, reuse=reuse):
            
            #< Define your network body here. Please make sure you don't use any layers created elsewhere >
            
            self.network = tf.keras.models.Sequential()
            self.network.add(keras.layers.Conv2D(filters=16, kernel_size=3, strides=1, activation='relu'))
            self.network.add(keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, activation='relu'))
            self.network.add(keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='relu'))
            self.network.add(keras.layers.Flatten(data_format = None))
            self.network.add(keras.layers.Dense(256, activation='relu'))
            self.network.add(keras.layers.Dense(n_actions, activation='softmax'))
            
            
            
            
            # prepare a graph for agent step
            self.state_t = tf.placeholder('float32', [None,] + list(state_shape))
            self.qvalues_t = self.get_symbolic_qvalues(self.state_t)
            
        self.weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)
        self.epsilon = epsilon

    def get_symbolic_qvalues(self, state_t):
        """takes agent's observation, returns qvalues. Both are tf Tensors"""
        #< apply your network layers here >                 
        self.network.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
        
        #qvalues = < symbolic tensor for q-values >
        qvalues = tf.placeholder('float32', list((state_t.shape[1]*state_t.shape[2], state_t[-1])))

        # ^^^^^^^^ This is where I am having trouble. I am not sure if this is the right way to implement a symbolic q-value ^^^^
        
        assert tf.is_numeric_tensor(qvalues) and qvalues.shape.ndims == 2, \
            "please return 2d tf tensor of qvalues [you got %s]" % repr(qvalues)
        assert int(qvalues.shape[1]) == n_actions
        
        return qvalues
    
    def get_qvalues(self, state_t):
        """Same as symbolic step except it operates on numpy arrays"""
        sess = tf.get_default_session()
        return sess.run(self.qvalues_t, {self.state_t: state_t})
    
    def sample_actions(self, qvalues):
        """pick actions given qvalues. Uses epsilon-greedy exploration strategy. """
        epsilon = self.epsilon
        batch_size, n_actions = qvalues.shape
        random_actions = np.random.choice(n_actions, size=batch_size)
        best_actions = qvalues.argmax(axis=-1)
        should_explore = np.random.choice([0, 1], batch_size, p = [1-epsilon, epsilon])
        return np.where(should_explore, random_actions, best_actions)
        
        
        
agent = DQNAgent("dqn_agent", state_dim, n_actions, epsilon=0.5)
sess.run(tf.global_variables_initializer())        
       
